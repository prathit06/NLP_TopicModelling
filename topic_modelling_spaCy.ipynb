{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-cloud-translate\n",
    "#!pip install --upgrade google-cloud-translate --force-reinstall\n",
    "#reset session\n",
    "#!pip install requests\n",
    "#reset session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate                                                                                      #lib to use google translation api\n",
    "import pandas as pd                                                                                                     #lib to work with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('/content/datalab/data_csv/data.csv')                                                            #reading data.csv using pandas\n",
    "head = data_raw.head(573)                                                   #len(data_csv)=573                          #converting comments in the csv file to list format for further computation\n",
    "#comments = data_raw[\"Comment\"].tolist()                                                                           \n",
    "#head_list = head[\"Comment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_comments = []                                                                                                #empty list to store the translated comments\n",
    " \n",
    "def translate_text(text,target='en'): \n",
    "    \"\"\"\n",
    "    Function to translate the comments\n",
    "    \n",
    "    args: text: str\n",
    "          target: str, default='en'\n",
    "    \"\"\"\n",
    "    text = text.decode('utf-8')                                                                      \n",
    "    translate_client = translate.Client()                                                                               #creating an object for the translation api                                                                       \n",
    "    result = translate_client.translate(text,target_language=target)                                                    #using the translation object                                              \n",
    "    translated_comments.append(result)                                                                                  #storing results in translated_comments                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = head[\"Comment\"].apply(translate_text)                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(translated_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy \n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                                                                                                            #lib to use the spacy nlp tool\n",
    "nlp = spacy.load(\"en\")                                                                                                  #loading the spacy nlp pipeline for pre processing\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"translatedText\"]=temp[\"translatedText\"].apply(nlp)                                                        #applying spacy nlp pipeline on the translatedText\n",
    "#processed_comments = temp.translatedText                                                                               #final processed comments\n",
    "#processed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [A, name, for, my, next, trap, record, ?, The,...\n",
       "1                                   [Hermosaaa, 😻, 😍, 😍]\n",
       "2                                          [Chochinibar]\n",
       "3                                      [Cod, croquettes]\n",
       "4                                                    [💚]\n",
       "5                                           [Bollodrama]\n",
       "6      [Hi, ,, I&#39;m, a, bowler, and, I, throw, eve...\n",
       "7                                    [Sara&#39;s, Pasta]\n",
       "8                                               [Amazon]\n",
       "9                                         [Bollitrapcao]\n",
       "10                            [Payment, for, my, garden]\n",
       "11                    [They, copy, us, :(, @inescrespoo]\n",
       "12                                             [Tractra]\n",
       "13                           [The, DE, -, SEE, -, WOMAN]\n",
       "14                                        [Bollotrap, 😂]\n",
       "15                        [here, comes, the, bollodrama]\n",
       "16                                        [chumi, power]\n",
       "17                                [industrial, pastries]\n",
       "18     [It, is, not, &, quot;to, palaver&quot, ;, wha...\n",
       "19     [@esthervg, _, indignadaaaa, ,, that, we, have...\n",
       "20                                     [Pussy, power, 🤣]\n",
       "21                                                   [😍]\n",
       "22     [If, Marta, hesitates, you, shut, up, and, ass...\n",
       "23                           [The, beauty, of, Cádiz, 😘]\n",
       "24                          [Eat, pussy, not, palm, oil]\n",
       "25     [If, I&#39;m, not, a, baker, ,, how, do, I, ha...\n",
       "26                                             [Trapped]\n",
       "27     [I&#39;m, Marta, and, I, give, you, what, you,...\n",
       "28                               [Bolleramente, between]\n",
       "29                      [@, k.dlarosa, JAJAJAJJJJAJAJJJ]\n",
       "                             ...                        \n",
       "220                                  [@thali_mlsb, ☹, ️]\n",
       "221    [@, raquelsm_95, ☹, ️, There, are, many, ,, bu...\n",
       "222                                     [To, my, dog, 😂]\n",
       "223                                       [A, tod, @, s]\n",
       "224                                 [A, @chioth05, ❤, ✨]\n",
       "225                                [@andreeaglezg, 😘, 😘]\n",
       "226    [Nobody, because, I&#39;m, more, alone, than, ...\n",
       "227                      [Bebesitaaaa, @, 1rayhernandez]\n",
       "228      [@, live.and, leave, behind, you, I, love, you]\n",
       "229                              [Toelrrato, @, noaaa_3]\n",
       "230                             [@liidwi, xeeeee, 🤣, ️️]\n",
       "231        [What, are, you, doing, here, ?, JAJAJAJJAJA]\n",
       "232    [JAJAJAJAJA, learning, from, the, things, of, ...\n",
       "233             [Mamona, you, are, Jurao, jajajajjajaja]\n",
       "234                                       [@marywendy83]\n",
       "235    [@, lindez92, like, that, ?, ?, ?, ?, Jajajaja...\n",
       "236    [I, would, like, to, kiss, this, girl, @i_am_m...\n",
       "237         [Love, me, uncle, 😔, JAJAJJAJA, @estheergil]\n",
       "238                [@, otitasone94, I, already, want, 💋]\n",
       "239                                      [@geconjota, 💕]\n",
       "240               [to, the, pillow, vaya, 😂, @laaura_vl]\n",
       "241        [I, leave, you, if, you, want, @beacoterillo]\n",
       "242                   [@janius, _, oki, @paulanoviajanx]\n",
       "243    [@milyagv, 😍, 😍, 😍, 😍, my, everything, ,, I, l...\n",
       "244            [@, chioth05, and, I, love, you, 😚, 😚, 💕]\n",
       "245                   [@zzmiriamjimenezz, I, adore, you]\n",
       "246    [@clara__roma, all, life, and, missing, lives, 💜]\n",
       "247                           [@, aroalozano95, baia, 😏]\n",
       "248                 [@, helenacorona98, I, like, kisses]\n",
       "249                                   [@sauvage_soul, 😍]\n",
       "Name: translatedText, Length: 250, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processed_comments=processed_comments.apply(lambda x: map(str, x))                                                      #converting comments from spacy tokens to string\n",
    "temp[\"translatedText\"]=temp[\"translatedText\"].apply(lambda x: map(str,x))\n",
    "temp.translatedText.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [a, name, for, my, next, trap, record, ?, the,...\n",
       "1                                   [hermosaaa, 😻, 😍, 😍]\n",
       "2                                          [chochinibar]\n",
       "3                                      [cod, croquettes]\n",
       "4                                                    [💚]\n",
       "5                                           [bollodrama]\n",
       "6      [hi, ,, i&#39;m, a, bowler, and, i, throw, eve...\n",
       "7                                    [sara&#39;s, pasta]\n",
       "8                                               [amazon]\n",
       "9                                         [bollitrapcao]\n",
       "10                            [payment, for, my, garden]\n",
       "11                    [they, copy, us, :(, @inescrespoo]\n",
       "12                                             [tractra]\n",
       "13                           [the, de, -, see, -, woman]\n",
       "14                                        [bollotrap, 😂]\n",
       "15                        [here, comes, the, bollodrama]\n",
       "16                                        [chumi, power]\n",
       "17                                [industrial, pastries]\n",
       "18     [it, is, not, &, quot;to, palaver&quot, ;, wha...\n",
       "19     [@esthervg, _, indignadaaaa, ,, that, we, have...\n",
       "20                                     [pussy, power, 🤣]\n",
       "21                                                   [😍]\n",
       "22     [if, marta, hesitates, you, shut, up, and, ass...\n",
       "23                           [the, beauty, of, cádiz, 😘]\n",
       "24                          [eat, pussy, not, palm, oil]\n",
       "25     [if, i&#39;m, not, a, baker, ,, how, do, i, ha...\n",
       "26                                             [trapped]\n",
       "27     [i&#39;m, marta, and, i, give, you, what, you,...\n",
       "28                               [bolleramente, between]\n",
       "29                      [@, k.dlarosa, jajajajjjjajajjj]\n",
       "                             ...                        \n",
       "220                                  [@thali_mlsb, ☹, ️]\n",
       "221    [@, raquelsm_95, ☹, ️, there, are, many, ,, bu...\n",
       "222                                     [to, my, dog, 😂]\n",
       "223                                       [a, tod, @, s]\n",
       "224                                 [a, @chioth05, ❤, ✨]\n",
       "225                                [@andreeaglezg, 😘, 😘]\n",
       "226    [nobody, because, i&#39;m, more, alone, than, ...\n",
       "227                      [bebesitaaaa, @, 1rayhernandez]\n",
       "228      [@, live.and, leave, behind, you, i, love, you]\n",
       "229                              [toelrrato, @, noaaa_3]\n",
       "230                             [@liidwi, xeeeee, 🤣, ️️]\n",
       "231        [what, are, you, doing, here, ?, jajajajjaja]\n",
       "232    [jajajajaja, learning, from, the, things, of, ...\n",
       "233             [mamona, you, are, jurao, jajajajjajaja]\n",
       "234                                       [@marywendy83]\n",
       "235    [@, lindez92, like, that, ?, ?, ?, ?, jajajaja...\n",
       "236    [i, would, like, to, kiss, this, girl, @i_am_m...\n",
       "237         [love, me, uncle, 😔, jajajjaja, @estheergil]\n",
       "238                [@, otitasone94, i, already, want, 💋]\n",
       "239                                      [@geconjota, 💕]\n",
       "240               [to, the, pillow, vaya, 😂, @laaura_vl]\n",
       "241        [i, leave, you, if, you, want, @beacoterillo]\n",
       "242                   [@janius, _, oki, @paulanoviajanx]\n",
       "243    [@milyagv, 😍, 😍, 😍, 😍, my, everything, ,, i, l...\n",
       "244            [@, chioth05, and, i, love, you, 😚, 😚, 💕]\n",
       "245                   [@zzmiriamjimenezz, i, adore, you]\n",
       "246    [@clara__roma, all, life, and, missing, lives, 💜]\n",
       "247                           [@, aroalozano95, baia, 😏]\n",
       "248                 [@, helenacorona98, i, like, kisses]\n",
       "249                                   [@sauvage_soul, 😍]\n",
       "Name: translatedText, Length: 250, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lower(strings):  \n",
    "  \"\"\"\n",
    "  Function to set the strings of translatedText column in lower case\n",
    "    \n",
    "  args: text: str\n",
    "  \"\"\"\n",
    "  stem_text = [string.lower() for string in strings]\n",
    "  return stem_text\n",
    "temp[\"translatedText\"] = temp[\"translatedText\"].apply(lower)\n",
    "temp.translatedText.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stop_words_remove(strings):\n",
    "  stop = [string for string in strings if string not in stop_words]\n",
    "  return stop\n",
    "temp[\"translatedText\"] = temp[\"translatedText\"].apply(stop_words_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [name, next, trap, record, cover, already, 👇, ...\n",
       "1                                   [hermosaaa, 😻, 😍, 😍]\n",
       "2                                          [chochinibar]\n",
       "3                                      [cod, croquettes]\n",
       "4                                                    [💚]\n",
       "5                                           [bollodrama]\n",
       "6      [hi, i&#39;m, bowler, throw, everything, moves...\n",
       "7                                    [sara&#39;s, pasta]\n",
       "8                                               [amazon]\n",
       "9                                         [bollitrapcao]\n",
       "10                                     [payment, garden]\n",
       "11                          [copy, us, :(, @inescrespoo]\n",
       "12                                             [tractra]\n",
       "13                                     [de, -, -, woman]\n",
       "14                                        [bollotrap, 😂]\n",
       "15                                   [comes, bollodrama]\n",
       "16                                        [chumi, power]\n",
       "17                                [industrial, pastries]\n",
       "18              [quot;to, palaver&quot, aunt, brings, 🔊]\n",
       "19                  [@esthervg, _, indignadaaaa, two, 😂]\n",
       "20                                     [pussy, power, 🤣]\n",
       "21                                                   [😍]\n",
       "22               [marta, hesitates, shut, assimilate, 😂]\n",
       "23                                    [beauty, cádiz, 😘]\n",
       "24                               [eat, pussy, palm, oil]\n",
       "25                                 [i&#39;m, baker, bun]\n",
       "26                                             [trapped]\n",
       "27     [i&#39;m, marta, need, bollitrap, i&#39;ll, pu...\n",
       "28                                        [bolleramente]\n",
       "29                         [k.dlarosa, jajajajjjjajajjj]\n",
       "                             ...                        \n",
       "170    [shurri, beautiful, world, tkm, wapo, @guillor...\n",
       "171                                     [@sara_schia, 🌹]\n",
       "172                                       [@annalipi, 🌹]\n",
       "173                                                  [💜]\n",
       "174                                    [@andreatt, 💜, 💜]\n",
       "175                                            [wall, 😭]\n",
       "176                                          [davidba98]\n",
       "177                                        [@alexadulfi]\n",
       "178                                                  [😍]\n",
       "179                               [@danelys_veliz, ❤, ️]\n",
       "180                                       [@sofiinatale]\n",
       "181                                       [mothers, xfa]\n",
       "182                             [@alba_miraa, one, 😂, 🙌]\n",
       "183    [guapaas, 💕, 💕, ❤, ️, want, another, diaaaaa, ...\n",
       "184                           [andreadelmar_27, ❤, ️, 🔥]\n",
       "185                            [lucre_24, life, ❤, ️, 🔥]\n",
       "186        [y.jensen.r, ❤, ️, 😍, ❤, ️, 😍, ❤, ️, 😍, ❤, ️]\n",
       "187                         [@alba_miraa, 🤣, 🤣, 🤣, 🤣, 🙌]\n",
       "188                                      [embrace, life]\n",
       "189                          [steaks, smashed, potatoes]\n",
       "190                                      [❤, ️, 🥂, ❤, ️]\n",
       "191                             [nobody, /, left, years]\n",
       "192                                                  [😭]\n",
       "193                                                  [😭]\n",
       "194                                                  [😭]\n",
       "195                                                  [😭]\n",
       "196                             [natasha.garrido, ❤, ti]\n",
       "197                                                   []\n",
       "198                       [@naopf, _, tiiii, baby, 💜, 😍]\n",
       "199                                    [@celiatraver, 💥]\n",
       "Name: translatedText, Length: 200, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.translatedText.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u'@',u',',u'🤣',u'.',u'😍',u'quiqu',u'🙃',u'😍😢',u'😘😍👌',u'💜',u'i',u'🇨🇱',u'🔝🔝🔝',u'?',u'😍😍😍😍👏👏👏👏',u'😔',u'#',u'❤😍',u'🇨🇦',u'😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹😍🌹',u'😍😍😍❤️',u'😊',u'😭😭❤️❤️',u'🤣',u'❤️👏🏼',u'🤤',u'💜💜💜',u'🔊',u'❤️💘',u'\\ufe0f',u'👏👏',u'🔥🔥🔥🔥🔥',u'😍😍😍🔝🔝❤️',u'❄️🇨🇦❄️',u'👇🏼🙅',u';',u'✔',u'!',u'&',u'i',u'39',u'*',u'much',u'',u'💚',u'☹️',u'❤️🖤',u'give',u'know',u'like',u'see',u'mona❤️❤️❤️',u'jo',u'awwww😍',u'🌹',u'hermosas😍',u'...',u':',u'(', u')',u'..',u'😍',u'😍😍',u'😍😍😍',u'♥',u'❤️',u'😘',u'💖',u'😂',u'love',u'💙',u'beauti',u'❤️🔥',u'quot',u'❤️❤️❤️❤️',u'quiqu',u'😭😭',u'❤️❤️❤️',u'😍😍😍😍',u'❤',u'💕',u'i',u'8',u'',u'would',u'😍❤️',u'😭',u'😂😂']\n",
    "def my_stop_words_remove(strings):\n",
    "  stop = [string for string in strings if string not in my_stop_words]\n",
    "  return stop\n",
    "#temp[\"translatedText\"] = temp[\"translatedText\"].apply(my_stop_words_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_comments = temp.translatedText\n",
    "#!pip install gensim\n",
    "import gensim                                                                                                         #lib to apply topic modelling                                                                                                                                                    \n",
    "import gensim.corpora.dictionary                                                                                      #lib to map between words and their integer ids\n",
    "dictionary = gensim.corpora.Dictionary(processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_comments]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary)                              #applying LDA using bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052*\"😍\" + 0.027*\"💜\" + 0.024*\"🔥\" + 0.011*\"beautiful\" + 0.010*\"😭\" + 0.008*\"dog\" + 0.008*\"little\" + 0.008*\"it&#39;s\" + 0.006*\"think\" + 0.005*\"great\"\n",
      "0.161*\"😍\" + 0.016*\"😘\" + 0.016*\"😂\" + 0.010*\"beautiful\" + 0.010*\"💕\" + 0.008*\"tra\" + 0.008*\"👏\" + 0.007*\"one\" + 0.006*\"life\" + 0.005*\"❤\"\n",
      "0.033*\"😍\" + 0.014*\"❤\" + 0.011*\"💖\" + 0.010*\"take\" + 0.009*\"life\" + 0.009*\"️\" + 0.009*\"it&#39;s\" + 0.009*\"jajajaja\" + 0.009*\"women\" + 0.009*\"quique\"\n",
      "0.073*\"😍\" + 0.032*\"🌹\" + 0.024*\"beautiful\" + 0.022*\"😘\" + 0.021*\"😭\" + 0.017*\"😂\" + 0.017*\"❤\" + 0.010*\"🤗\" + 0.009*\"nobody\" + 0.009*\"i&#39;m\"\n",
      "0.157*\"️\" + 0.138*\"❤\" + 0.020*\"beautiful\" + 0.016*\"♥\" + 0.015*\"😍\" + 0.012*\"😭\" + 0.007*\"♀\" + 0.007*\"🏼\" + 0.007*\"_\" + 0.007*\"🔥\"\n",
      "0.011*\"quique\" + 0.010*\"🖤\" + 0.009*\"😂\" + 0.008*\"️\" + 0.007*\"girlfriend\" + 0.007*\"@luuciaperrg\" + 0.007*\"photo\" + 0.007*\"❤\" + 0.006*\"us\" + 0.006*\"arrived\"\n",
      "0.027*\"😍\" + 0.022*\"️\" + 0.018*\"_\" + 0.016*\"💙\" + 0.014*\"❤\" + 0.013*\"nice\" + 0.011*\"👏\" + 0.011*\"beautiful\" + 0.011*\"😻\" + 0.011*\"🖤\"\n",
      "0.034*\"️\" + 0.030*\"❤\" + 0.029*\"💞\" + 0.015*\"😍\" + 0.011*\"quique\" + 0.011*\"💓\" + 0.009*\"life\" + 0.008*\"💕\" + 0.008*\"good\" + 0.007*\"i&#39;m\"\n",
      "0.049*\"👏\" + 0.026*\"😍\" + 0.025*\"😂\" + 0.022*\"🤣\" + 0.018*\"💙\" + 0.017*\"️\" + 0.016*\"💛\" + 0.012*\"🇦\" + 0.010*\"🇷\" + 0.009*\"best\"\n",
      "0.016*\"😍\" + 0.016*\"😢\" + 0.012*\"-\" + 0.012*\"🔝\" + 0.011*\"️\" + 0.008*\"die\" + 0.008*\"quique\" + 0.008*\"·\" + 0.008*\"🔥\" + 0.008*\"🧡\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, lda_model_bow.num_topics):                                                                           #printing different topics in the document\n",
    "  print lda_model_bow.print_topic(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models                                                                                     #lib to import modules to use tf,idf models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary)                           #applyting LDA using tf,idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021*\"beautiful\" + 0.020*\"❤\" + 0.018*\"️\" + 0.008*\"🤤\" + 0.008*\"🔥\" + 0.007*\"💘\" + 0.007*\"😍\" + 0.007*\"🖤\" + 0.006*\"photo\" + 0.006*\"💖\"\n",
      "0.021*\"️\" + 0.021*\"❤\" + 0.016*\"💜\" + 0.012*\"🖤\" + 0.010*\"😂\" + 0.010*\"😍\" + 0.010*\"💖\" + 0.008*\"girlfriend\" + 0.007*\"💛\" + 0.007*\"😭\"\n",
      "0.043*\"️\" + 0.042*\"❤\" + 0.016*\"😍\" + 0.013*\"🔥\" + 0.010*\"♥\" + 0.008*\"wonderful\" + 0.006*\"bed\" + 0.006*\"quique\" + 0.005*\"💓\" + 0.005*\"😭\"\n",
      "0.044*\"😍\" + 0.010*\"beautiful\" + 0.010*\"❤\" + 0.008*\"one\" + 0.008*\"🤣\" + 0.007*\"admire\" + 0.007*\"💕\" + 0.006*\"🐶\" + 0.006*\"couple\" + 0.006*\"ti\"\n",
      "0.022*\"beautiful\" + 0.020*\"😘\" + 0.017*\"😍\" + 0.009*\"thanks\" + 0.009*\"😭\" + 0.009*\"hand\" + 0.007*\"👍\" + 0.007*\"video\" + 0.006*\"moms\" + 0.006*\"precious\"\n",
      "0.012*\"❤\" + 0.010*\"️\" + 0.009*\"👏\" + 0.008*\"💕\" + 0.006*\"power\" + 0.006*\"pussy\" + 0.006*\"😍\" + 0.006*\"😂\" + 0.005*\"amazon\" + 0.005*\"@bilitz7\"\n",
      "0.022*\"💙\" + 0.010*\"️\" + 0.010*\"😢\" + 0.010*\"💜\" + 0.009*\"😭\" + 0.009*\"😍\" + 0.008*\"❤\" + 0.007*\"dog\" + 0.007*\"😂\" + 0.007*\"_\"\n",
      "0.029*\"😍\" + 0.019*\"👏\" + 0.009*\"️\" + 0.009*\"bolleramente\" + 0.007*\"pussy\" + 0.007*\"kiss\" + 0.006*\"❤\" + 0.006*\"♥\" + 0.006*\"💕\" + 0.006*\"😘\"\n",
      "0.010*\"🤣\" + 0.009*\"bollodrama\" + 0.008*\"amazing\" + 0.007*\"😂\" + 0.006*\"🌈\" + 0.006*\"mariamarua2003\" + 0.006*\"patriarchates\" + 0.006*\"street\" + 0.006*\"sun\" + 0.006*\"beautiful\"\n",
      "0.043*\"😍\" + 0.020*\"😭\" + 0.012*\"😂\" + 0.011*\"wall\" + 0.010*\"💚\" + 0.009*\"🔝\" + 0.006*\"bedroom\" + 0.005*\"jajajajjaja\" + 0.005*\"@m_t_c_3\" + 0.005*\"parliament\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, lda_model_tfidf.num_topics):                                                                          #printing different topics in the document\n",
    "  print lda_model_tfidf.print_topic(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
