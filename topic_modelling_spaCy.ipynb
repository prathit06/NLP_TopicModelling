{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-cloud-translate\n",
    "#!pip install --upgrade google-cloud-translate --force-reinstall\n",
    "#reset session\n",
    "#!pip install requests\n",
    "#reset session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate                                                                                      #lib to use google translation api\n",
    "import pandas as pd                                                                                                     #lib to work with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('/content/datalab/data_csv/data.csv')                                                            #reading data.csv using pandas\n",
    "head = data_raw.head(573)                                                   #len(data_csv)=573                          #converting comments in the csv file to list format for further computation\n",
    "#comments = data_raw[\"Comment\"].tolist()                                                                           \n",
    "#head_list = head[\"Comment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_comments = []                                                                                                #empty list to store the translated comments\n",
    " \n",
    "def translate_text(text,target='en'): \n",
    "    \"\"\"\n",
    "    Function to translate the comments\n",
    "    \n",
    "    args: text: str\n",
    "          target: str, default='en'\n",
    "    \"\"\"\n",
    "    text = text.decode('utf-8')                                                                      \n",
    "    translate_client = translate.Client()                                                                               #creating an object for the translation api                                                                       \n",
    "    result = translate_client.translate(text,target_language=target)                                                    #using the translation object                                              \n",
    "    translated_comments.append(result)                                                                                  #storing results in translated_comments                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = head[\"Comment\"].apply(translate_text)                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(translated_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy \n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                                                                                                            #lib to use the spacy nlp tool\n",
    "nlp = spacy.load(\"en\")                                                                                                  #loading the spacy nlp pipeline for pre processing\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"translatedText\"]=temp[\"translatedText\"].apply(nlp)                                                        #applying spacy nlp pipeline on the translatedText\n",
    "#processed_comments = temp.translatedText                                                                               #final processed comments\n",
    "#processed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [A, name, for, my, next, trap, record, ?, The,...\n",
       "1                                   [Hermosaaa, ğŸ˜», ğŸ˜, ğŸ˜]\n",
       "2                                          [Chochinibar]\n",
       "3                                      [Cod, croquettes]\n",
       "4                                                    [ğŸ’š]\n",
       "5                                           [Bollodrama]\n",
       "6      [Hi, ,, I&#39;m, a, bowler, and, I, throw, eve...\n",
       "7                                    [Sara&#39;s, Pasta]\n",
       "8                                               [Amazon]\n",
       "9                                         [Bollitrapcao]\n",
       "10                            [Payment, for, my, garden]\n",
       "11                    [They, copy, us, :(, @inescrespoo]\n",
       "12                                             [Tractra]\n",
       "13                           [The, DE, -, SEE, -, WOMAN]\n",
       "14                                        [Bollotrap, ğŸ˜‚]\n",
       "15                        [here, comes, the, bollodrama]\n",
       "16                                        [chumi, power]\n",
       "17                                [industrial, pastries]\n",
       "18     [It, is, not, &, quot;to, palaver&quot, ;, wha...\n",
       "19     [@esthervg, _, indignadaaaa, ,, that, we, have...\n",
       "20                                     [Pussy, power, ğŸ¤£]\n",
       "21                                                   [ğŸ˜]\n",
       "22     [If, Marta, hesitates, you, shut, up, and, ass...\n",
       "23                           [The, beauty, of, CÃ¡diz, ğŸ˜˜]\n",
       "24                          [Eat, pussy, not, palm, oil]\n",
       "25     [If, I&#39;m, not, a, baker, ,, how, do, I, ha...\n",
       "26                                             [Trapped]\n",
       "27     [I&#39;m, Marta, and, I, give, you, what, you,...\n",
       "28                               [Bolleramente, between]\n",
       "29                      [@, k.dlarosa, JAJAJAJJJJAJAJJJ]\n",
       "                             ...                        \n",
       "220                                  [@thali_mlsb, â˜¹, ï¸]\n",
       "221    [@, raquelsm_95, â˜¹, ï¸, There, are, many, ,, bu...\n",
       "222                                     [To, my, dog, ğŸ˜‚]\n",
       "223                                       [A, tod, @, s]\n",
       "224                                 [A, @chioth05, â¤, âœ¨]\n",
       "225                                [@andreeaglezg, ğŸ˜˜, ğŸ˜˜]\n",
       "226    [Nobody, because, I&#39;m, more, alone, than, ...\n",
       "227                      [Bebesitaaaa, @, 1rayhernandez]\n",
       "228      [@, live.and, leave, behind, you, I, love, you]\n",
       "229                              [Toelrrato, @, noaaa_3]\n",
       "230                             [@liidwi, xeeeee, ğŸ¤£, ï¸ï¸]\n",
       "231        [What, are, you, doing, here, ?, JAJAJAJJAJA]\n",
       "232    [JAJAJAJAJA, learning, from, the, things, of, ...\n",
       "233             [Mamona, you, are, Jurao, jajajajjajaja]\n",
       "234                                       [@marywendy83]\n",
       "235    [@, lindez92, like, that, ?, ?, ?, ?, Jajajaja...\n",
       "236    [I, would, like, to, kiss, this, girl, @i_am_m...\n",
       "237         [Love, me, uncle, ğŸ˜”, JAJAJJAJA, @estheergil]\n",
       "238                [@, otitasone94, I, already, want, ğŸ’‹]\n",
       "239                                      [@geconjota, ğŸ’•]\n",
       "240               [to, the, pillow, vaya, ğŸ˜‚, @laaura_vl]\n",
       "241        [I, leave, you, if, you, want, @beacoterillo]\n",
       "242                   [@janius, _, oki, @paulanoviajanx]\n",
       "243    [@milyagv, ğŸ˜, ğŸ˜, ğŸ˜, ğŸ˜, my, everything, ,, I, l...\n",
       "244            [@, chioth05, and, I, love, you, ğŸ˜š, ğŸ˜š, ğŸ’•]\n",
       "245                   [@zzmiriamjimenezz, I, adore, you]\n",
       "246    [@clara__roma, all, life, and, missing, lives, ğŸ’œ]\n",
       "247                           [@, aroalozano95, baia, ğŸ˜]\n",
       "248                 [@, helenacorona98, I, like, kisses]\n",
       "249                                   [@sauvage_soul, ğŸ˜]\n",
       "Name: translatedText, Length: 250, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processed_comments=processed_comments.apply(lambda x: map(str, x))                                                      #converting comments from spacy tokens to string\n",
    "temp[\"translatedText\"]=temp[\"translatedText\"].apply(lambda x: map(str,x))\n",
    "temp.translatedText.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [a, name, for, my, next, trap, record, ?, the,...\n",
       "1                                   [hermosaaa, ğŸ˜», ğŸ˜, ğŸ˜]\n",
       "2                                          [chochinibar]\n",
       "3                                      [cod, croquettes]\n",
       "4                                                    [ğŸ’š]\n",
       "5                                           [bollodrama]\n",
       "6      [hi, ,, i&#39;m, a, bowler, and, i, throw, eve...\n",
       "7                                    [sara&#39;s, pasta]\n",
       "8                                               [amazon]\n",
       "9                                         [bollitrapcao]\n",
       "10                            [payment, for, my, garden]\n",
       "11                    [they, copy, us, :(, @inescrespoo]\n",
       "12                                             [tractra]\n",
       "13                           [the, de, -, see, -, woman]\n",
       "14                                        [bollotrap, ğŸ˜‚]\n",
       "15                        [here, comes, the, bollodrama]\n",
       "16                                        [chumi, power]\n",
       "17                                [industrial, pastries]\n",
       "18     [it, is, not, &, quot;to, palaver&quot, ;, wha...\n",
       "19     [@esthervg, _, indignadaaaa, ,, that, we, have...\n",
       "20                                     [pussy, power, ğŸ¤£]\n",
       "21                                                   [ğŸ˜]\n",
       "22     [if, marta, hesitates, you, shut, up, and, ass...\n",
       "23                           [the, beauty, of, cÃ¡diz, ğŸ˜˜]\n",
       "24                          [eat, pussy, not, palm, oil]\n",
       "25     [if, i&#39;m, not, a, baker, ,, how, do, i, ha...\n",
       "26                                             [trapped]\n",
       "27     [i&#39;m, marta, and, i, give, you, what, you,...\n",
       "28                               [bolleramente, between]\n",
       "29                      [@, k.dlarosa, jajajajjjjajajjj]\n",
       "                             ...                        \n",
       "220                                  [@thali_mlsb, â˜¹, ï¸]\n",
       "221    [@, raquelsm_95, â˜¹, ï¸, there, are, many, ,, bu...\n",
       "222                                     [to, my, dog, ğŸ˜‚]\n",
       "223                                       [a, tod, @, s]\n",
       "224                                 [a, @chioth05, â¤, âœ¨]\n",
       "225                                [@andreeaglezg, ğŸ˜˜, ğŸ˜˜]\n",
       "226    [nobody, because, i&#39;m, more, alone, than, ...\n",
       "227                      [bebesitaaaa, @, 1rayhernandez]\n",
       "228      [@, live.and, leave, behind, you, i, love, you]\n",
       "229                              [toelrrato, @, noaaa_3]\n",
       "230                             [@liidwi, xeeeee, ğŸ¤£, ï¸ï¸]\n",
       "231        [what, are, you, doing, here, ?, jajajajjaja]\n",
       "232    [jajajajaja, learning, from, the, things, of, ...\n",
       "233             [mamona, you, are, jurao, jajajajjajaja]\n",
       "234                                       [@marywendy83]\n",
       "235    [@, lindez92, like, that, ?, ?, ?, ?, jajajaja...\n",
       "236    [i, would, like, to, kiss, this, girl, @i_am_m...\n",
       "237         [love, me, uncle, ğŸ˜”, jajajjaja, @estheergil]\n",
       "238                [@, otitasone94, i, already, want, ğŸ’‹]\n",
       "239                                      [@geconjota, ğŸ’•]\n",
       "240               [to, the, pillow, vaya, ğŸ˜‚, @laaura_vl]\n",
       "241        [i, leave, you, if, you, want, @beacoterillo]\n",
       "242                   [@janius, _, oki, @paulanoviajanx]\n",
       "243    [@milyagv, ğŸ˜, ğŸ˜, ğŸ˜, ğŸ˜, my, everything, ,, i, l...\n",
       "244            [@, chioth05, and, i, love, you, ğŸ˜š, ğŸ˜š, ğŸ’•]\n",
       "245                   [@zzmiriamjimenezz, i, adore, you]\n",
       "246    [@clara__roma, all, life, and, missing, lives, ğŸ’œ]\n",
       "247                           [@, aroalozano95, baia, ğŸ˜]\n",
       "248                 [@, helenacorona98, i, like, kisses]\n",
       "249                                   [@sauvage_soul, ğŸ˜]\n",
       "Name: translatedText, Length: 250, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lower(strings):  \n",
    "  \"\"\"\n",
    "  Function to set the strings of translatedText column in lower case\n",
    "    \n",
    "  args: text: str\n",
    "  \"\"\"\n",
    "  stem_text = [string.lower() for string in strings]\n",
    "  return stem_text\n",
    "temp[\"translatedText\"] = temp[\"translatedText\"].apply(lower)\n",
    "temp.translatedText.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stop_words_remove(strings):\n",
    "  stop = [string for string in strings if string not in stop_words]\n",
    "  return stop\n",
    "temp[\"translatedText\"] = temp[\"translatedText\"].apply(stop_words_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [name, next, trap, record, cover, already, ğŸ‘‡, ...\n",
       "1                                   [hermosaaa, ğŸ˜», ğŸ˜, ğŸ˜]\n",
       "2                                          [chochinibar]\n",
       "3                                      [cod, croquettes]\n",
       "4                                                    [ğŸ’š]\n",
       "5                                           [bollodrama]\n",
       "6      [hi, i&#39;m, bowler, throw, everything, moves...\n",
       "7                                    [sara&#39;s, pasta]\n",
       "8                                               [amazon]\n",
       "9                                         [bollitrapcao]\n",
       "10                                     [payment, garden]\n",
       "11                          [copy, us, :(, @inescrespoo]\n",
       "12                                             [tractra]\n",
       "13                                     [de, -, -, woman]\n",
       "14                                        [bollotrap, ğŸ˜‚]\n",
       "15                                   [comes, bollodrama]\n",
       "16                                        [chumi, power]\n",
       "17                                [industrial, pastries]\n",
       "18              [quot;to, palaver&quot, aunt, brings, ğŸ”Š]\n",
       "19                  [@esthervg, _, indignadaaaa, two, ğŸ˜‚]\n",
       "20                                     [pussy, power, ğŸ¤£]\n",
       "21                                                   [ğŸ˜]\n",
       "22               [marta, hesitates, shut, assimilate, ğŸ˜‚]\n",
       "23                                    [beauty, cÃ¡diz, ğŸ˜˜]\n",
       "24                               [eat, pussy, palm, oil]\n",
       "25                                 [i&#39;m, baker, bun]\n",
       "26                                             [trapped]\n",
       "27     [i&#39;m, marta, need, bollitrap, i&#39;ll, pu...\n",
       "28                                        [bolleramente]\n",
       "29                         [k.dlarosa, jajajajjjjajajjj]\n",
       "                             ...                        \n",
       "170    [shurri, beautiful, world, tkm, wapo, @guillor...\n",
       "171                                     [@sara_schia, ğŸŒ¹]\n",
       "172                                       [@annalipi, ğŸŒ¹]\n",
       "173                                                  [ğŸ’œ]\n",
       "174                                    [@andreatt, ğŸ’œ, ğŸ’œ]\n",
       "175                                            [wall, ğŸ˜­]\n",
       "176                                          [davidba98]\n",
       "177                                        [@alexadulfi]\n",
       "178                                                  [ğŸ˜]\n",
       "179                               [@danelys_veliz, â¤, ï¸]\n",
       "180                                       [@sofiinatale]\n",
       "181                                       [mothers, xfa]\n",
       "182                             [@alba_miraa, one, ğŸ˜‚, ğŸ™Œ]\n",
       "183    [guapaas, ğŸ’•, ğŸ’•, â¤, ï¸, want, another, diaaaaa, ...\n",
       "184                           [andreadelmar_27, â¤, ï¸, ğŸ”¥]\n",
       "185                            [lucre_24, life, â¤, ï¸, ğŸ”¥]\n",
       "186        [y.jensen.r, â¤, ï¸, ğŸ˜, â¤, ï¸, ğŸ˜, â¤, ï¸, ğŸ˜, â¤, ï¸]\n",
       "187                         [@alba_miraa, ğŸ¤£, ğŸ¤£, ğŸ¤£, ğŸ¤£, ğŸ™Œ]\n",
       "188                                      [embrace, life]\n",
       "189                          [steaks, smashed, potatoes]\n",
       "190                                      [â¤, ï¸, ğŸ¥‚, â¤, ï¸]\n",
       "191                             [nobody, /, left, years]\n",
       "192                                                  [ğŸ˜­]\n",
       "193                                                  [ğŸ˜­]\n",
       "194                                                  [ğŸ˜­]\n",
       "195                                                  [ğŸ˜­]\n",
       "196                             [natasha.garrido, â¤, ti]\n",
       "197                                                   []\n",
       "198                       [@naopf, _, tiiii, baby, ğŸ’œ, ğŸ˜]\n",
       "199                                    [@celiatraver, ğŸ’¥]\n",
       "Name: translatedText, Length: 200, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.translatedText.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u'@',u',',u'ğŸ¤£',u'.',u'ğŸ˜',u'quiqu',u'ğŸ™ƒ',u'ğŸ˜ğŸ˜¢',u'ğŸ˜˜ğŸ˜ğŸ‘Œ',u'ğŸ’œ',u'i',u'ğŸ‡¨ğŸ‡±',u'ğŸ”ğŸ”ğŸ”',u'?',u'ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘',u'ğŸ˜”',u'#',u'â¤ğŸ˜',u'ğŸ‡¨ğŸ‡¦',u'ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹ğŸ˜ğŸŒ¹',u'ğŸ˜ğŸ˜ğŸ˜â¤ï¸',u'ğŸ˜Š',u'ğŸ˜­ğŸ˜­â¤ï¸â¤ï¸',u'ğŸ¤£',u'â¤ï¸ğŸ‘ğŸ¼',u'ğŸ¤¤',u'ğŸ’œğŸ’œğŸ’œ',u'ğŸ”Š',u'â¤ï¸ğŸ’˜',u'\\ufe0f',u'ğŸ‘ğŸ‘',u'ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥',u'ğŸ˜ğŸ˜ğŸ˜ğŸ”ğŸ”â¤ï¸',u'â„ï¸ğŸ‡¨ğŸ‡¦â„ï¸',u'ğŸ‘‡ğŸ¼ğŸ™…',u';',u'âœ”',u'!',u'&',u'i',u'39',u'*',u'much',u'',u'ğŸ’š',u'â˜¹ï¸',u'â¤ï¸ğŸ–¤',u'give',u'know',u'like',u'see',u'monaâ¤ï¸â¤ï¸â¤ï¸',u'jo',u'awwwwğŸ˜',u'ğŸŒ¹',u'hermosasğŸ˜',u'...',u':',u'(', u')',u'..',u'ğŸ˜',u'ğŸ˜ğŸ˜',u'ğŸ˜ğŸ˜ğŸ˜',u'â™¥',u'â¤ï¸',u'ğŸ˜˜',u'ğŸ’–',u'ğŸ˜‚',u'love',u'ğŸ’™',u'beauti',u'â¤ï¸ğŸ”¥',u'quot',u'â¤ï¸â¤ï¸â¤ï¸â¤ï¸',u'quiqu',u'ğŸ˜­ğŸ˜­',u'â¤ï¸â¤ï¸â¤ï¸',u'ğŸ˜ğŸ˜ğŸ˜ğŸ˜',u'â¤',u'ğŸ’•',u'i',u'8',u'',u'would',u'ğŸ˜â¤ï¸',u'ğŸ˜­',u'ğŸ˜‚ğŸ˜‚']\n",
    "def my_stop_words_remove(strings):\n",
    "  stop = [string for string in strings if string not in my_stop_words]\n",
    "  return stop\n",
    "#temp[\"translatedText\"] = temp[\"translatedText\"].apply(my_stop_words_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_comments = temp.translatedText\n",
    "#!pip install gensim\n",
    "import gensim                                                                                                         #lib to apply topic modelling                                                                                                                                                    \n",
    "import gensim.corpora.dictionary                                                                                      #lib to map between words and their integer ids\n",
    "dictionary = gensim.corpora.Dictionary(processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_comments]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary)                              #applying LDA using bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052*\"ğŸ˜\" + 0.027*\"ğŸ’œ\" + 0.024*\"ğŸ”¥\" + 0.011*\"beautiful\" + 0.010*\"ğŸ˜­\" + 0.008*\"dog\" + 0.008*\"little\" + 0.008*\"it&#39;s\" + 0.006*\"think\" + 0.005*\"great\"\n",
      "0.161*\"ğŸ˜\" + 0.016*\"ğŸ˜˜\" + 0.016*\"ğŸ˜‚\" + 0.010*\"beautiful\" + 0.010*\"ğŸ’•\" + 0.008*\"tra\" + 0.008*\"ğŸ‘\" + 0.007*\"one\" + 0.006*\"life\" + 0.005*\"â¤\"\n",
      "0.033*\"ğŸ˜\" + 0.014*\"â¤\" + 0.011*\"ğŸ’–\" + 0.010*\"take\" + 0.009*\"life\" + 0.009*\"ï¸\" + 0.009*\"it&#39;s\" + 0.009*\"jajajaja\" + 0.009*\"women\" + 0.009*\"quique\"\n",
      "0.073*\"ğŸ˜\" + 0.032*\"ğŸŒ¹\" + 0.024*\"beautiful\" + 0.022*\"ğŸ˜˜\" + 0.021*\"ğŸ˜­\" + 0.017*\"ğŸ˜‚\" + 0.017*\"â¤\" + 0.010*\"ğŸ¤—\" + 0.009*\"nobody\" + 0.009*\"i&#39;m\"\n",
      "0.157*\"ï¸\" + 0.138*\"â¤\" + 0.020*\"beautiful\" + 0.016*\"â™¥\" + 0.015*\"ğŸ˜\" + 0.012*\"ğŸ˜­\" + 0.007*\"â™€\" + 0.007*\"ğŸ¼\" + 0.007*\"_\" + 0.007*\"ğŸ”¥\"\n",
      "0.011*\"quique\" + 0.010*\"ğŸ–¤\" + 0.009*\"ğŸ˜‚\" + 0.008*\"ï¸\" + 0.007*\"girlfriend\" + 0.007*\"@luuciaperrg\" + 0.007*\"photo\" + 0.007*\"â¤\" + 0.006*\"us\" + 0.006*\"arrived\"\n",
      "0.027*\"ğŸ˜\" + 0.022*\"ï¸\" + 0.018*\"_\" + 0.016*\"ğŸ’™\" + 0.014*\"â¤\" + 0.013*\"nice\" + 0.011*\"ğŸ‘\" + 0.011*\"beautiful\" + 0.011*\"ğŸ˜»\" + 0.011*\"ğŸ–¤\"\n",
      "0.034*\"ï¸\" + 0.030*\"â¤\" + 0.029*\"ğŸ’\" + 0.015*\"ğŸ˜\" + 0.011*\"quique\" + 0.011*\"ğŸ’“\" + 0.009*\"life\" + 0.008*\"ğŸ’•\" + 0.008*\"good\" + 0.007*\"i&#39;m\"\n",
      "0.049*\"ğŸ‘\" + 0.026*\"ğŸ˜\" + 0.025*\"ğŸ˜‚\" + 0.022*\"ğŸ¤£\" + 0.018*\"ğŸ’™\" + 0.017*\"ï¸\" + 0.016*\"ğŸ’›\" + 0.012*\"ğŸ‡¦\" + 0.010*\"ğŸ‡·\" + 0.009*\"best\"\n",
      "0.016*\"ğŸ˜\" + 0.016*\"ğŸ˜¢\" + 0.012*\"-\" + 0.012*\"ğŸ”\" + 0.011*\"ï¸\" + 0.008*\"die\" + 0.008*\"quique\" + 0.008*\"Â·\" + 0.008*\"ğŸ”¥\" + 0.008*\"ğŸ§¡\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, lda_model_bow.num_topics):                                                                           #printing different topics in the document\n",
    "  print lda_model_bow.print_topic(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models                                                                                     #lib to import modules to use tf,idf models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary)                           #applyting LDA using tf,idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021*\"beautiful\" + 0.020*\"â¤\" + 0.018*\"ï¸\" + 0.008*\"ğŸ¤¤\" + 0.008*\"ğŸ”¥\" + 0.007*\"ğŸ’˜\" + 0.007*\"ğŸ˜\" + 0.007*\"ğŸ–¤\" + 0.006*\"photo\" + 0.006*\"ğŸ’–\"\n",
      "0.021*\"ï¸\" + 0.021*\"â¤\" + 0.016*\"ğŸ’œ\" + 0.012*\"ğŸ–¤\" + 0.010*\"ğŸ˜‚\" + 0.010*\"ğŸ˜\" + 0.010*\"ğŸ’–\" + 0.008*\"girlfriend\" + 0.007*\"ğŸ’›\" + 0.007*\"ğŸ˜­\"\n",
      "0.043*\"ï¸\" + 0.042*\"â¤\" + 0.016*\"ğŸ˜\" + 0.013*\"ğŸ”¥\" + 0.010*\"â™¥\" + 0.008*\"wonderful\" + 0.006*\"bed\" + 0.006*\"quique\" + 0.005*\"ğŸ’“\" + 0.005*\"ğŸ˜­\"\n",
      "0.044*\"ğŸ˜\" + 0.010*\"beautiful\" + 0.010*\"â¤\" + 0.008*\"one\" + 0.008*\"ğŸ¤£\" + 0.007*\"admire\" + 0.007*\"ğŸ’•\" + 0.006*\"ğŸ¶\" + 0.006*\"couple\" + 0.006*\"ti\"\n",
      "0.022*\"beautiful\" + 0.020*\"ğŸ˜˜\" + 0.017*\"ğŸ˜\" + 0.009*\"thanks\" + 0.009*\"ğŸ˜­\" + 0.009*\"hand\" + 0.007*\"ğŸ‘\" + 0.007*\"video\" + 0.006*\"moms\" + 0.006*\"precious\"\n",
      "0.012*\"â¤\" + 0.010*\"ï¸\" + 0.009*\"ğŸ‘\" + 0.008*\"ğŸ’•\" + 0.006*\"power\" + 0.006*\"pussy\" + 0.006*\"ğŸ˜\" + 0.006*\"ğŸ˜‚\" + 0.005*\"amazon\" + 0.005*\"@bilitz7\"\n",
      "0.022*\"ğŸ’™\" + 0.010*\"ï¸\" + 0.010*\"ğŸ˜¢\" + 0.010*\"ğŸ’œ\" + 0.009*\"ğŸ˜­\" + 0.009*\"ğŸ˜\" + 0.008*\"â¤\" + 0.007*\"dog\" + 0.007*\"ğŸ˜‚\" + 0.007*\"_\"\n",
      "0.029*\"ğŸ˜\" + 0.019*\"ğŸ‘\" + 0.009*\"ï¸\" + 0.009*\"bolleramente\" + 0.007*\"pussy\" + 0.007*\"kiss\" + 0.006*\"â¤\" + 0.006*\"â™¥\" + 0.006*\"ğŸ’•\" + 0.006*\"ğŸ˜˜\"\n",
      "0.010*\"ğŸ¤£\" + 0.009*\"bollodrama\" + 0.008*\"amazing\" + 0.007*\"ğŸ˜‚\" + 0.006*\"ğŸŒˆ\" + 0.006*\"mariamarua2003\" + 0.006*\"patriarchates\" + 0.006*\"street\" + 0.006*\"sun\" + 0.006*\"beautiful\"\n",
      "0.043*\"ğŸ˜\" + 0.020*\"ğŸ˜­\" + 0.012*\"ğŸ˜‚\" + 0.011*\"wall\" + 0.010*\"ğŸ’š\" + 0.009*\"ğŸ”\" + 0.006*\"bedroom\" + 0.005*\"jajajajjaja\" + 0.005*\"@m_t_c_3\" + 0.005*\"parliament\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, lda_model_tfidf.num_topics):                                                                          #printing different topics in the document\n",
    "  print lda_model_tfidf.print_topic(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
